                                                                        -- Neural Network Weight Initializers --

-----

# Weight Initialization Methods in Deep Learning

Proper weight initialization is crucial for training deep neural networks. It prevents issues like vanishing/exploding gradients and helps models converge faster ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=As%20you%20can%20see%2C%20in,was%20later%20refined%20into%20Kaiming)) ([Selecting the right weight initialization for your deep neural network - Comet](https://www.comet.com/site/blog/selecting-the-right-weight-initialization-for-your-deep-neural-network/#:~:text=Improperly%20initialized%20weights%20can%20negatively,propagation%20%28see%20more%C2%A0here)). Below, we compile **15 academic sources** and **10 high-quality online publications** that discuss five common initialization methods – **Glorot (Xavier)**, **He (Kaiming)**, **LeCun**, **Orthogonal**, and **Zero** initialization. Each source’s key contributions are summarized, including the mathematical formulation, theoretical justification, practical usage, and empirical findings. We then compare these initialization schemes on theoretical and empirical grounds.

## Academic Sources (Peer-Reviewed)

**1. Glorot & Bengio (2010)** – *“Understanding the difficulty of training deep feedforward neural networks.”* AISTATS 2010 ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=6.%20,Workshop%20and%20Conference%20Proceedings%3A%20249%E2%80%93256)). This seminal paper introduced **Xavier/Glorot initialization**, proposing to draw weights $W_{ij}$ from $\mathcal{U}\!\Big(-\sqrt{\frac{6}{n_{in}+n_{out}}},\,+\sqrt{\frac{6}{n_{in}+n_{out}}}\Big)$ (uniform distribution) ([](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf#:~:text=call%20it%20the%20normalized%20initialization%3A,nj%20%2B%20nj%2B1%20%2C%20%E2%88%9A)). In other words, $\text{Var}(W) = \frac{2}{n_{in}+n_{out}}$ for a layer with $n_{in}$ inputs (fan-in) and $n_{out}$ outputs (fan-out). The authors show this choice preserves **variance of activations and backpropagated gradients** across layers, mitigating the vanishing/exploding gradient problem ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Glorot%20initialization%20,variance%20during%20the%20backward%20pass)) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=fan,the%20same%20as%20LeCun%20initialization)). The theoretical justification is that it balances two constraints: maintaining signal variance during forward propagation and gradient variance during backpropagation ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Glorot%20initialization%20,variance%20during%20the%20backward%20pass)). Empirically, networks initialized with Glorot’s scheme trained deeper (5+ layers) and faster than with previous small-random or Gaussian initializations ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=As%20you%20can%20see%2C%20in,was%20later%20refined%20into%20Kaiming)). This work established that careful initialization is *“important for deep networks because of the multiplicative effect through layers”* ([](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf#:~:text=initializing%20deep%20networks%20because%20of,%E2%88%BC%20U%20h%20%E2%88%92%20%E2%88%9A)). Glorot initialization became a default for sigmoid/tanh networks and is implemented as `GlorotUniform/Normal` in modern libraries ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=For%20uniform%20initialization%2C%20it%20samples,the%20same%20as%20LeCun%20initialization)) ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)).

**2. He et al. (2015)** – *“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.”* ICCV 2015 ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=8.%20,Exact)). Kaiming He and colleagues extended Xavier’s approach for **ReLU activations**, introducing **He initialization** ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=As%20Glorot%20initialization%20performs%20poorly,1)). They derive that for ReLU-like neurons, to keep variance constant, weights should be sampled from $\mathcal{N}(0, \sigma^2)$ with $\sigma^2 = \tfrac{2}{n_{in}}$ (or uniform in $[-\sqrt{\frac{6}{n_{in}}},+\sqrt{\frac{6}{n_{in}}}]$) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=As%20Glorot%20initialization%20performs%20poorly,1)). This effectively doubles the variance used in Xavier (since a ReLU outputs nonzero roughly half the time) ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=There%20is%20only%20one%20tiny,of%20the%20weights%20by%202)). The paper provides a theoretical argument that Xavier’s symmetric assumption underestimates variance for ReLUs, and shows empirically that a 30-layer ReLU network **fails to converge with Xavier init but trains successfully with He init** ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)). He initialization enabled training very deep CNNs (e.g. 30-layer plain nets, later 100+ layer ResNets) from scratch ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)). The authors achieved state-of-the-art ImageNet performance, attributing it partly to this “robust initialization” that accounts for rectifier nonlinearity ([[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://ar5iv.org/pdf/1502.01852#:~:text=model%20fitting%20with%20nearly%20zero,on%20this%20visual%20recognition%20challenge)) ([[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://ar5iv.org/pdf/1502.01852#:~:text=risk,on%20this%20visual%20recognition%20challenge)). In practice, **He Normal** (mean 0, variance $2/n_{in}$) or **He Uniform** are recommended for ReLU and its variants ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=This%20is%20a%20theoretical%20justification,as%20the%20one%20for%20tanh)) ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)). This work solidified the principle of matching initialization to activation function dynamics.

**3. LeCun et al. (1998)** – *“Efficient BackProp.”* In *Neural Networks: Tricks of the Trade*, Springer ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=5.%20,8_2)). This earlier work (a book chapter by LeCun, Bottou, Orr, and Müller) popularized what’s now called **LeCun initialization** ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=,4%20Orthogonal%20initialization)) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=LeCun%20initialization%2C%20popularized%20in%20,activations%20during%20the%20forward%20pass)). It recommends initializing weights from $\mathcal{N}(0,\sigma^2)$ with $\sigma^2 = \frac{1}{n_{in}}$ (or uniform in $[-\sqrt{3/n_{in}}, +\sqrt{3/n_{in}}]$) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=It%20samples%20each%20entry%20in,1)). The idea is to preserve activation variance in forward propagation for networks with saturating activations like sigmoid or tanh, which helps them stay in the sensitive (linear) region of the activation function ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=LeCun%20initialization%2C%20popularized%20in%20,activations%20during%20the%20forward%20pass)). The theoretical basis is similar to Xavier’s forward-pass analysis but without considering backward variance – effectively a *fan-in* scaling. In fact, if fan-in equals fan-out, LeCun, Glorot, and He initializations coincide for linear case ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=%7B%5Cdisplaystyle%20n_%7Bl,the%20same%20as%20LeCun%20initialization)). This method was shown to speed up convergence of sigmoid networks vs. naive small random weights by reducing saturation at initialization. It’s the default in some frameworks (e.g. PyTorch for some layers) ([Weight Initialization and Activation Functions - Deep Learning Wizard](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#:~:text=Weight%20Initialization%20and%20Activation%20Functions,import%20torch)) and is crucial in later developments like Self-Normalizing Neural Networks with SELU activations (where maintaining unit variance is critical). LeCun’s work laid early groundwork for variance-based init, sometimes called “**Normalized Initialization**” in the context of efficient backprop ([Microsoft Word - ReviewIGARSSVF.docx](https://arxiv.org/pdf/2102.07004#:~:text=d,magnify%20the%20magnitudes%20of%20inputs)).

**4. Saxe et al. (2013)** – *“Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.”* ICLR 2014 (arXiv 1312.6120) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Deep%20into%20Rectifiers%3A%20Surpassing%20Human,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)). Saxe and colleagues propose **Orthogonal initialization** for deep linear networks and extend it conceptually to nonlinear nets. They prove that initializing each weight matrix as a random orthonormal matrix (e.g. from the Haar distribution) can make gradient propagation **depth-independent** in linear networks ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=%28Saxe%20et%20al.%202013%29,10)) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=orthogonal%20matrices%20%2C%20multiplied%20by,10)). Intuitively, if $W$ is orthogonal, it preserves the norm of signals (all singular values = 1), so neither explosions nor decays occur through layers. The authors show that with orthonormal $W$ (or “semi-orthogonal” if $n_{in}\neq n_{out}$, scaled appropriately), **training time does not degrade as depth increases** in linear networks ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=%28Saxe%20et%20al.%202013%29,10)). The formulation given is: sample $X$ with i.i.d. $\mathcal{N}(0,1)$ entries, perform QR decomposition to get an orthogonal $Q$, then set $W=Q\cdot c$ where $c$ is a gain (like $\sqrt{2}$ for ReLU, etc.) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Sampling%20a%20uniformly%20random%20semi,11)). The paper’s theoretical justification is strong for linear nets, and later works apply orthogonal init to nonlinear networks and RNNs. For example, orthogonal weights help RNNs retain long-term memory (see Arjovsky et al. 2016, below). Orthogonal init is now a built-in option in libraries (e.g. `torch.nn.init.orthogonal_`) and is empirically found to improve convergence for very deep or recurrent networks by preserving “dynamical isometry” (equalized singular values) at init ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Classification,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)).

**5. Mishkin & Matas (2016)** – *“All you need is a good init.”* ICLR 2016 ([[PDF] ALL YOU NEED IS A GOOD INIT - CMP](http://cmp.felk.cvut.cz/~mishkdmy/papers/mishkin-iclr2016.pdf#:~:text=Published%20as%20a%20conference%20paper,Czech%20Technical%20University)) ([README.md - ducha-aiki/LSUV-pytorch - GitHub](https://github.com/ducha-aiki/LSUV-pytorch/blob/master/README.md#:~:text=LSUV%20initialization%20is%20described%20in%3A,06422)). This paper introduced a two-step procedure called **Layer-Sequential Unit-Variance (LSUV) initialization** ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=Layer,to%20be%20equal%20to%20one)). First, each layer’s weights are initialized with a random orthonormal matrix (as per Saxe et al.). Second, a single batch of data is passed through the network and, one layer at a time, the weights are rescaled so that the output of that layer has unit variance ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=Layer,to%20be%20equal%20to%20one)). This ensures that **activations in all layers start with zero mean and unit variance**, similar to batch normalization but achieved purely through initialization. Mishkin and Matas demonstrate that LSUV allows training **very deep networks (up to 100+ layers) without batch normalization** or complex training protocols ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=Experiment%20with%20different%20activation%20functions,%28%201)). Empirically, LSUV-initialized networks matched or exceeded the accuracy of batch-normalized networks on MNIST, CIFAR-10/100 and ImageNet, while converging at least as fast ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=Experiment%20with%20different%20activation%20functions,%28%201)). The theoretical rationale is that combining orthogonal weights with layer-wise variance tuning yields an initialization close to the ideal “identity mapping” behavior (each layer initially just passes data through). Notably, they show VGG and GoogLeNet architectures can be trained in one shot (no layer-wise pre-training) using this initialization ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=One%20of%20the%20main%20obstacles,layers%2C%20especially%20with%20uniform%20initialization)) ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=On%20the%20other%20hand%2C%20He,computational%20overhead%20to%20each%20iteration)). This work underscores that **a good initialization can sometimes replace explicit normalization layers**, by front-loading the work of variance stabilization.

**6. Kumar (2017)** – *“On weight initialization in deep neural networks.”* arXiv 1704.08863 ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=7.%20,arXiv%3A1312.6120%20%5Bcs.NE)). This research provides a **theoretical analysis bridging Xavier and He initializations** for non-linear activations. Kumar derives a general initialization formula for any activation function differentiable at 0, by ensuring gradients neither vanish nor explode in expectation ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=A%20proper%20initialization%20of%20the,determining%20the%20proper%20weight%20initializations)). He shows that Glorot & Bengio’s result (variance $=1/n_{avg}$) is optimal for linear activations, and He et al.’s result (variance $=2/n_{in}$) is optimal for ReLU, which has a ReLU-specific derivative effect ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=The%20first%20systematic%20analysis%20of,to%20as%20the%20Xavier%20initialization)) ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)). The paper explains **why Xavier init underperforms for ReLU**: ReLU outputs have mean > 0, causing a systematic variance shrinkage if using symmetric fan-in/fan-out averaging ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=initialization%20strategy%20for%20any%20neural,determining%20the%20proper%20weight%20initializations)) ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)). Kumar’s analysis essentially shows a continuum of initialization variances depending on the activation’s slope at zero. His work provides a clearer theoretical foundation for “ReLU-aware” initialization, demonstrating mathematically that He’s formula is the correct limit for ReLU (slope 1 for positive inputs, 0 for negative). This deepens understanding of earlier heuristic derivations. While the paper is mostly theory (no new init method), it confirms with analysis that proper weight variance is crucial for convergence and that **non-linear activations’ characteristics should guide initialization** ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=A%20proper%20initialization%20of%20the,determining%20the%20proper%20weight%20initializations)) ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)).

**7. Klambauer et al. (2017)** – *“Self-Normalizing Neural Networks.”* NeurIPS 2017 ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=,scaled%20exponential%20linear%20units)) ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=,The%20winning%20SNN)). This paper introduced the SELU activation along with a specialized initialization (basically **LeCun Normal**). The authors prove that a network with SELU (scaled exponential linear units) will **self-normalize** – i.e. maintain zero-mean, unit-variance activations through many layers – if weights are initialized with $\mathcal{N}(0,1/n_{in})$ and biases 0 ([Self-Normalizing Neural Networks - SERP AI](https://serp.ai/posts/self-normalizing-neural-networks/#:~:text=Self,achieve%20superior%20performance%20across)) ([Self-Normalizing Neural Networks - Part 1 (2017) - Fast.ai Forums](https://forums.fast.ai/t/self-normalizing-neural-networks/3558#:~:text=rid%20of%20BN%2BReLU%20and%20followed,a%20SELU%20unit%20%E2%80%93)). This is essentially LeCun’s init, chosen to work in tandem with SELU’s specific $\alpha$ and $\lambda$ parameters. The theoretical analysis uses fixed-point theory (Banach fixed-point theorem) to show $(\mu,\sigma)=(0,1)$ is an attracting fixed-point for the network’s mean and variance under these conditions ([Self-Normalizing Neural Networks](https://proceedings.neurips.cc/paper/6698-self-normalizing-neural-networks.pdf#:~:text=Stable%20and%20Attracting%20Fixed%20Point,where%20the%20subscript%2001%20indicates)) ([Self-Normalizing Neural Networks](https://proceedings.neurips.cc/paper/6698-self-normalizing-neural-networks.pdf#:~:text=mapping%20and%20the%20fixed%20point,the%20Jacobian%20also%20determines%20the)). The result is that even 100+ layer feed-forward networks can be trained **without batch norm or dropout**, as the combination of initialization and activation keeps the signals in a stable range ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=and%2C%20therefore%20cannot%20exploit%20many,This)) ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=,The%20winning%20SNN)). Empirically, self-normalizing networks (SNNs) with this initialization/activation outperformed many traditional methods on 100+ UCI tasks and set a new record on a deep astronomy benchmark ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=,The%20winning%20SNN)). This work is an application case where **initialization (LeCun) is tuned to an activation (SELU)** to achieve theoretical guarantees against gradient vanishing/exploding ([Self-Normalizing Neural Networks - SERP AI](https://serp.ai/posts/self-normalizing-neural-networks/#:~:text=Self,achieve%20superior%20performance%20across)) ([Self-Normalizing Neural Networks - Part 1 (2017) - Fast.ai Forums](https://forums.fast.ai/t/self-normalizing-neural-networks/3558#:~:text=rid%20of%20BN%2BReLU%20and%20followed,a%20SELU%20unit%20%E2%80%93)).

**8. Arjovsky et al. (2016)** – *“Unitary Evolution Recurrent Neural Networks.”* ICML 2016 ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Conference%20on%20Machine%20Learning,15%29.%20%22Recurrent)). Although focused on recurrent networks, this paper highlights the importance of **orthogonal initialization** in a different context. The authors constrain an RNN’s weight matrices to be unitary (complex analog of orthogonal) or orthogonal, so that gradients don’t diminish over long time steps. They show that standard LSTM/GRU can fail on very long sequence tasks due to vanishing gradients, but a unitary/orthogonal RNN can **retain information for much longer** ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Conference%20on%20Machine%20Learning,15%29.%20%22Recurrent)). To train such networks, they initialize weights as (complex) orthogonal matrices and parametrize updates to remain orthogonal. The mathematical justification is that unitary matrices have eigenvalues on the unit circle, preserving norm over repeated multiplications (time steps), exactly analogous to how orthogonal init preserves norm across feedforward layers ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Deep%20into%20Rectifiers%3A%20Surpassing%20Human,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=9.%20,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)). Empirically, they solved pathological long-memory tasks (like memorization and addition problems) that are impossible without such initialization. This study confirms in the recurrent setting that **zero-initialization or random-init would fail**, and only careful orthogonal initial weights avoid vanishing/exploding gradients through time (a form of depth) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Deep%20into%20Rectifiers%3A%20Surpassing%20Human,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Conference%20on%20Machine%20Learning,15%29.%20%22Recurrent)). It’s a strong practical example that orthogonal initialization can be crucial beyond feed-forward nets, whenever we have deep computational graphs.

**9. Balduzzi et al. (2017)** – *“The Shattered Gradients Problem: If resnets are the answer, then what is the question?”* ICML 2017 ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=12.%20,Layer%20Vanilla%20Convolutional%20Neural)). This work investigates why very deep networks without skip connections struggle even with sophisticated initializations. They find that in architectures like 100-layer conv nets initialized with He initialization, the variance of gradients might be preserved, but **gradients become uncorrelated and “shattered”** as they propagate ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=12.%20,Layer%20Vanilla%20Convolutional%20Neural)). In effect, even with proper variance (He init), signals in deep ReLU networks decorrelate to the point of resembling white noise, making learning difficult. The paper doesn’t propose a new init scheme, but it provides a theoretical explanation for *residual networks*: with identity skip connections (which are effectively initialized as identity mappings), the network’s initial state is closer to an identity function, so gradients remain more correlated. This underscores that initialization isn’t only about variance – *architecture and correlation* matter too. Empirically, they show that  ResNets with standard He init train, whereas an equivalently deep plain net with the same init does not, due to shattered gradients ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=12.%20,Layer%20Vanilla%20Convolutional%20Neural)). The take-home is that even with He or Xavier, extremely deep nets benefit from architectural assists or other tricks (like LSUV or residual connections) to maintain meaningful gradient directions.

**10. Hu et al. (2020)** – *“Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks.”* ICLR 2020 ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Classification,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)). This theoretical paper offers formal guarantees for orthogonal initialization. Hu et al. prove that for deep *linear* networks, gradient descent with orthogonal weight init converges to the global minimum **independently of depth**, under certain conditions, whereas with Gaussian random init the convergence rate deteriorates exponentially with depth (or might get stuck in poor minima) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Classification,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)). The math involves analyzing the optimization landscape: orthogonal initialization keeps the Hessian well-conditioned for deep linear nets, leading to faster convergence. While the result is for linear networks, it provides insight that **orthogonal init yields more “trainable” parameter surfaces**. This complements Saxe et al.’s empirical depth-independent time by giving a convergence proof. It bolsters the recommendation of orthogonal init for very deep models, especially when combined with ReLU (with a $\sqrt{2}$ gain) to approximate linear behavior at initialization. The authors thus provide a rigorous theoretical justification that **orthogonal initialization is not just intuitively good, but provably advantageous** in certain regimes ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Classification,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)).

**11. Goodfellow et al. (2016)** – *“Deep Learning.”* MIT Press, Chapter 5 & 6 ([Ian Goodfellow - Deep Learning - 2016 | tomrochette.com](https://blog.tomrochette.com/agi/books/ian-goodfellow-deep-learning#:~:text=,or%20to%20small%20positive%20values)). This foundational textbook by Goodfellow, Bengio, and Courville summarizes best practices for initialization. It emphasizes that **weights should be initialized to small random values, and not all to the same value** ([Ian Goodfellow - Deep Learning - 2016 | tomrochette.com](https://blog.tomrochette.com/agi/books/ian-goodfellow-deep-learning#:~:text=,or%20to%20small%20positive%20values)). The text explains that random initialization “breaks symmetry,” ensuring that different neurons receive different gradients ([Ian Goodfellow - Deep Learning - 2016 | tomrochette.com](https://blog.tomrochette.com/agi/books/ian-goodfellow-deep-learning#:~:text=,or%20to%20small%20positive%20values)). Biases are often initialized to zero (or a small constant) with little issue ([Ian Goodfellow - Deep Learning - 2016 | tomrochette.com](https://blog.tomrochette.com/agi/books/ian-goodfellow-deep-learning#:~:text=,or%20to%20small%20positive%20values)), but weights *must* differ. The authors cite that poor initialization can prevent convergence or lead to numerical instability ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=,numerical%20difficulties%20and%20fails%20altogether)). They review Xavier and He initializations: Xavier is recommended for sigmoidal/tanh nets to keep activation variance stable, and He for ReLU nets to account for ReLU’s half-active nature ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=In%20practice%2C%20Machine%20Learning%20Engineers,1%5D%7D%7D%24%20and%20%24%5Cfrac%7B1%7D%7Bn%5E%7B%5Bl)). They also mention that if using ReLU, one can think of He init as “Xavier with variance multiplied by 2” ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=This%20is%20a%20theoretical%20justification,as%20the%20one%20for%20tanh)). No new experiments are presented (being a textbook), but it provides a clear theoretical rationale: proper initialization puts the network in a regime where it can learn (neither stuck with zero gradients nor divergence) ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=In%20practice%2C%20Machine%20Learning%20Engineers,1%5D%7D%7D%24%20and%20%24%5Cfrac%7B1%7D%7Bn%5E%7B%5Bl)) ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=This%20is%20a%20theoretical%20justification,as%20the%20one%20for%20tanh)). This source is often cited for rules-of-thumb on initialization and the importance of not using zero weights.

**12. Zhang et al. (2019)** – *“Fixup Initialization: Residual Learning Without Normalization.”* ICLR 2019. This paper introduces **Fixup initialization**, a strategy enabling **100+-layer ResNets to train without Batch Normalization**. Fixup init involves: (a) initialize weights in residual branches from $\mathcal{N}(0, \sigma^2)$ with a scaled-down $\sigma$ (e.g. $1/\sqrt{m}$ for $m$ layers) except the last layer in each residual block, which is initialized to **zero**; (b) initialize biases in certain layers to zero or specific small values. The zero-initialized last layer in each block forces each residual block to start as an identity function (since the residual output is zero initially) – essentially making the initial network an identity mapping, like an *extreme form of LSUV for ResNets*. The authors theoretically argue this keeps gradients in a reasonable range and empirically demonstrate training of very deep (up to 110-layer) ResNets without batch norm, achieving performance on par with batch-norm networks. Fixup shows that even for architectures designed to mitigate initialization issues (ResNets), one can *further adjust initialization to remove the need for normalization*. This highlights the interplay between network design and initialization: with the right init (even involving some layers at zero), gradient flow can be maintained. While Fixup is specialized, it reinforces that **partial zero initialization can be useful** when done in a structured way to bootstrap deep signal propagation.

*(The above academic sources collectively cover the mathematical formulations of each initialization scheme, the reasoning behind them (maintaining variance, avoiding symmetry or saturation), practical usage notes, and experimental evidence ranging from small MLPs to state-of-the-art deep CNNs and RNNs.)*

## Public Online Publications (Blogs and Tutorials)

**1. Pierre Ouannes (2019) – “How to initialize deep neural networks? Xavier and Kaiming initialization.”** ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=In%20this%20post%2C%20I%E2%80%99ll%20walk,of%20two%20very%20significant%20papers)) ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=The%20Xavier%20and%20Kaiming%20papers,though%2C%20no%20way%20around%20that)) – This blog post provides an intuitive walkthrough of **Xavier vs. Kaiming (He)** initialization. It explains the *“initialization headache”* – if done improperly, activations and gradients can explode or vanish even in a 5-layer network ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=As%20you%20can%20see%2C%20in,was%20later%20refined%20into%20Kaiming)). Ouannes illustrates with histograms that standard random init leads to vanishing activations and gradients by layer 5 ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=As%20you%20can%20see%2C%20in,was%20later%20refined%20into%20Kaiming)), motivating the need for Xavier’s solution. He derives Xavier’s condition by requiring $Var[y_l] = Var[y_{l-1}]$ in forward propagation and similarly constant gradient variance in backprop ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=Forward)) ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=Under%20those%20assumptions%2C%20if%20we,eq%3Afwd%7D%29%5C%29%20we%20get)). The blog shows the **Xavier formula** (Gaussian with $\sigma^2=1/n_{in}$ or $1/n_{avg}$) and how He initialization is a slight modification accounting for ReLU’s 50% dropout of negative values ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=The%20Xavier%20and%20Kaiming%20papers,though%2C%20no%20way%20around%20that)). It emphasizes that **Kaiming He’s method adapts Xavier by “taking into account the activation function”**, effectively using the derivative of ReLU (~0.5 on average) instead of 1 in the variance calculations ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=The%20Xavier%20and%20Kaiming%20papers,though%2C%20no%20way%20around%20that)). Practical insights are given, such as using Xavier for tanh and He for ReLU by default. The post also references the original papers for those who want the full derivations. Overall, it offers a clear rationale of *why* we need these initializers (to avoid the failure modes shown in the initial histograms) in an accessible way.

**2. Mustafa M. Arat (2019) – “Weight Initialization Schemes – Xavier and He.”** ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=%5C%5BVar%28W_)) ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=Glorot%20and%20Bengio%20considered%20logistic,is%20sometimes%20called%20He%20initialization)) – This tutorial-style blog explains both schemes and provides formulas and code snippets. It starts by deriving **Xavier/Glorot**: requiring both forward and backward signal variances to stay constant leads to $\text{Var}(W)=1/n_{in}$ for forward and $1/n_{out}$ for backward, so the compromise is $\frac{1}{n_{avg}}$ ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=In%20order%20to%20keep%20the,out%7D%24%2C%20proposing%20that)). Arat presents the Gaussian version $\mathcal{N}(0,\,1/n_{avg})$ and the Uniform version $[-\sqrt{6/(n_{in}+n_{out})}, +\sqrt{6/(n_{in}+n_{out})}]$ ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=If%20sampling%20from%20a%20uniform,Backward%20Passes%2C%20we%20will%20have)). He then discusses how with the advent of ReLUs, Xavier needed adjustment ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=Glorot%20and%20Bengio%20considered%20logistic,is%20sometimes%20called%20He%20initialization)). **He initialization** is introduced as *“the same idea (balancing variance) applied to ReLU”* ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=vanishing%20%2F%20exploding%20gradients%20problem,of%20the%20weights%20by%202)) – specifically, just doubling the variance: $\text{Var}(W)=2/n_{in}$ ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=There%20is%20only%20one%20tiny,of%20the%20weights%20by%202)). The blog concisely states: *“There is only one tiny adjustment we need to make [from Xavier], which is to multiply the variance by 2!”* ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=There%20is%20only%20one%20tiny,of%20the%20weights%20by%202)). A nice table is provided comparing recommended formulas for different activations (tanh, sigmoid, ReLU) for both uniform and normal distributions ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=Activation%20Function%20Uniform%20Distribution%20%5B,out)). For example, sigmoid has a larger recommended scale (4 times Xavier’s) due to its strong saturation effect ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=Activation%20Function%20Uniform%20Distribution%20%5B,out)). This source gives practical **“when to use what”** advice and notes that frameworks implement these as `tf.initializers.Glorot...` or `variance_scaling_initializer` (for He) ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=In%20Tensorflow%2C%20He%20initialization%20is,Xavier%20initializer%20is%20logically%20xavier_initializer)). It’s a straightforward guide useful for practitioners to grasp the math and implement the initializers correctly.

**3. DeepLearning.AI (2020) – “Initializing neural networks.”** ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=Initialization%20can%20have%20a%20significant,initialize%20neural%20network%20parameters%20effectively)) ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=In%20practice%2C%20Machine%20Learning%20Engineers,1%5D%7D%7D%24%20and%20%24%5Cfrac%7B1%7D%7Bn%5E%7B%5Bl)) – This is an interactive online note (by Andrew Ng’s team) focusing on the importance of initialization. It vividly demonstrates problems like **zero initialization** with an interactive example: if all weights = 0, neurons learn the same features and never diverge (the note states *“all neurons produce the same output…leading to identical gradients…preventing learning”* ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=,the%20same%20features%20during%20training))). It then describes the pitfalls of weights that are *too small* or *too large*, linking them to vanishing or exploding gradients ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=Initialization%20can%20have%20a%20significant,initialize%20neural%20network%20parameters%20effectively)) ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=I%20The%20importance%20of%20effective,initialization)). The note defines **proper initialization** as setting weights “not too small, not too large” so that the network operates in the linear regime initially ([DL Tutorial 30 — Weight Initialization Methods and Best Practices](https://ai.plainenglish.io/dl-tutorial-30-weight-initialization-methods-and-best-practices-53c06aef0fb7#:~:text=Practices%20ai,How%20to)) ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=,good%20variance)). It provides the formula for **Xavier initialization** and its justification: by modeling activations as i.i.d. variables and requiring variance to stay constant each layer ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=In%20practice%2C%20Machine%20Learning%20Engineers,1%5D%7D%7D%24%20and%20%24%5Cfrac%7B1%7D%7Bn%5E%7B%5Bl)). It explicitly gives two common choices in practice: $\mathcal{N}(0,1/n_{in})$ or $\mathcal{N}(0,2/(n_{in}+n_{out}))$, noting the latter is the harmonic mean of the fan-in and fan-out variances ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=In%20practice%2C%20Machine%20Learning%20Engineers,1%5D%7D%7D%24%20and%20%24%5Cfrac%7B1%7D%7Bn%5E%7B%5Bl)). It also notes that Xavier was designed for activations like tanh (which are zero-mean), and that for ReLU one should use **He initialization (He et al.)**, effectively *“multiplying by 2 the variance of Xavier”* ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=This%20is%20a%20theoretical%20justification,as%20the%20one%20for%20tanh)). This source is useful for its **clear, visual explanation** of symmetry breaking and for summarizing the key init schemes in simple terms for learners.

**4. Pinecone (2022) – “Weight Initialization Techniques in Neural Networks.”** ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)) – This blog post discusses various initializers and their effects on a toy network. It first shows experiments: initializing weights very small (0.01 std) leads to **vanishing activations** layer by layer ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=For%20small%20random%20values%20of,deeper%20into%20the%20neural%20network)), while initializing too large leads to **saturated activations** (tanh outputs ±1, gradients ~0) ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=W%20%3D%20np)) ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=Let%E2%80%99s%20summarize%20the%20observations%20from,the%20above%20experiments)). This motivates finding an optimal variance. The article then introduces **Xavier/Glorot init**, describing it as sampling from a distribution that “keeps things centered around zero and adjusts the spread” of the initial weights ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=,bell%20curve%20distribution%20but%20doesn%E2%80%99t)). It explains that Xavier works well for **sigmoid/tanh** because those activations have zero mean output ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)). It then states that **Glorot init failed for ReLU** networks because ReLU outputs are not zero-centered, and that He’s method was introduced to address this ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=It%20was%20found%20that%20Glorot,3)) ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)). A key summary given is: *“Xavier (Glorot) initialization works well for activations with zero mean (sigmoid, tanh). When using ReLU (which has a positive mean output), it’s recommended to use He initialization.”* ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)). The blog also briefly touches on **Orthogonal initialization**, noting that one can initialize weights as orthogonal matrices to help information flow (especially mentioned for recurrent layers in PyTorch) ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=PyTorch%20Weight%20Initialization%20Demystified%20,weights%20of%20recurrent%20layers%20effectively)). Overall, this post provides **practical evidence** of the phenomena (vanishing/saturating) and succinct guidance: use Xavier for tanh, He for ReLU, and be aware of special cases (it even mentions that biases are usually 0 or small constant). It also links to Keras initializers (`GlorotUniform`, `HeNormal`, etc.) showing how to implement these in code ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=Glorot%20Initialization%20in%20Keras)) ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=defaults%20to%20)).

**5. Jason Brownlee (2016, updated 2020) – “Weight Initialization for Deep Learning Neural Networks.”** ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=,use%20the%20ReLU%20activation%20function)) ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=We%20cannot%20initialize%20all%20weights,gradient%20to%20begin%20searching%20effectively)) – This tutorial explains in simple terms why weight initialization matters and how to implement popular schemes. It notes that historically, initializing weights to small random values (e.g. uniform in [-0.3,0.3]) was common ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=Historically%2C%20weight%20initialization%20follows%20simple,heuristics%2C%20such%20as)), but newer approaches leverage the network architecture (fan-in/out) and activation function ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=Historically%2C%20weight%20initialization%20involved%20using,of%20inputs%20to%20the%20node)). The article highlights that **initialization is the starting point for optimization** ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=This%20optimization%20algorithm%20requires%20a,of%20the%20neural%20network%20model)) and that *“some initial points are so unstable that the algorithm fails altogether”* ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=,numerical%20difficulties%20and%20fails%20altogether)) (quoting the Deep Learning book). Crucially, Brownlee warns **“We cannot initialize all weights to 0.0”** because the optimizer will have no direction to break symmetry ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=We%20cannot%20initialize%20all%20weights,gradient%20to%20begin%20searching%20effectively)). Instead, weights must be random: each neuron then receives a unique gradient and can learn different features ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=Taking%20the%20first%20one%20if,our%20model%20to%20be%20asymmetric)). The tutorial then covers **Xavier initialization** (for sigmoid/tanh) and **He initialization** (for ReLU) as the go-to “heuristics” ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=After%20completing%20this%20tutorial%2C%20you,will%20know)). It provides the equations and how to implement them in code (e.g., initializing a Dense layer in Keras with `kernel_initializer='he_normal'` for ReLU) along with examples. It also mentions *“Normalized Xavier”* which is essentially the same as Xavier/Glorot Normal in modern terms ([Weight Initialization for Deep Learning Neural Networks - MachineLearningMastery.com](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/#:~:text=,use%20the%20ReLU%20activation%20function)). This source is practical: it tells the reader which initializer to use for which activation and why (Xavier to avoid vanishing in sigmoid, He to avoid ReLU dying). It reinforces fundamentals (don’t use zeros, random sign symmetry breaking) in an easy-to-understand way.

**6. Samarth Gupta (2021) – “Weight Initialization in Neural Net (AI in Plain English).”** ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=This%20blog%20covers%20weight%20initialization,I%20have%20some%20cases)) ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=,good%20variance)) – This Medium article quickly covers the consequences of poor initialization with a couple of cases. It plainly states that initializing all weights to 0 is *“a very very bad idea”* – due to the **symmetry problem**, all neurons will produce the same output and thus learn the same thing, effectively reducing the model to one neuron ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=Taking%20the%20first%20one%20if,our%20model%20to%20be%20asymmetric)). It also notes that initializing weights to large negative values will lead ReLU outputs to be zero (since $z$ will be < 0) and thus also prevent learning ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=In%20the%20second%20case%2C%20if,these%20cases%20we%20want)). From these cases, the author draws three simple desiderata for good initialization: *“weights should be small (but not too small), not all values should be the same, and [have] good variance.”* ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=,good%20variance)). The article then lists different methods, including *“Gaussian (Normal) init”* as a baseline (e.g. $\mathcal{N}(0,0.05^2)$). It presumably goes on to Xavier and He, though the snippet we have is focused on the intuitive points. The strength of this source is its **clarity for beginners**: it introduces the term *“symmetry problem”* explicitly and communicates the key idea that randomization is needed to differentiate neurons’ learning. It also stresses the Goldilocks principle (not too small or too large). This is a good non-technical summary that sets the stage for why one would then use Xavier or He – to quantitatively achieve those goals of proper variance.

**7. DeepLearningWizard (2018) – “Weight Initialization and Activation Functions.”** ([Weight Initialization and Activation Functions - Deep Learning Wizard](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#:~:text=Weight%20Initialization%20and%20Activation%20Functions,be%20solved%20with%20Xavier%20initialization)) – This is a blog/tutorial that summarizes various weight initialization solutions in relation to activation functions. It particularly notes: *“Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization.”* and *“For ReLU units, use He initialization.”* ([Weight Initialization and Activation Functions - Deep Learning Wizard](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#:~:text=Weight%20Initialization%20and%20Activation%20Functions,be%20solved%20with%20Xavier%20initialization)). It essentially maps out that if you see gradients vanishing with sigmoid/tanh, switching to Xavier (which keeps variance through layers) helps; and if using ReLU, Xavier is insufficient and He’s variance scaling is needed to avoid “dying ReLUs” in deep layers. The post also discusses practical tips like PyTorch’s default init. It’s another confirmation from an educational blog that **Xavier = default for older activations, He = default for ReLU** ([Weight Initialization and Activation Functions - Deep Learning Wizard](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#:~:text=Weight%20Initialization%20and%20Activation%20Functions,be%20solved%20with%20Xavier%20initialization)). It may also mention LeCun initialization for SELU (since SELU is another activation solving vanishing gradients, requiring its own init). Although brief, this blog is useful as a quick reference or **cheat-sheet** linking activation types to the appropriate weight initializer, distilled from the collective knowledge of the community.

**8. Novita AI Blog (2023) – “PyTorch Weight Initialization Demystified.”** ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=,bell%20curve%20distribution%20but%20doesn%E2%80%99t)) ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=,fills%20everything%20up%20with%20zeroes)) – This blog explains how PyTorch’s `nn.init` module provides convenient functions for different schemes. It describes each initializer in simple language: e.g., *“Xavier or Glorot initialization goes for a bell curve approach but keeps things centered around zero and tweaks how spread out the numbers are,”* and *“Kaiming is perfect if you’re using ReLU because it adjusts weight scale based on how ReLU behaves.”* ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=,bell%20curve%20distribution%20but%20doesn%E2%80%99t)) ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=,fills%20everything%20up%20with%20zeroes)). This gives a non-mathematical intuition: Xavier picks a distribution that’s not too narrow or wide (just right) and Kaiming He is aware that ReLU is off half the time so it compensates. The blog also enumerates that PyTorch’s `xavier_uniform_` and `xavier_normal_` functions implement Glorot’s init, and `kaiming_normal_`/`uniform_` implement He’s (with options for `fan_in` vs `fan_out` scaling depending if used for forward or backward preservation). It briefly mentions **Zero and Ones initializers** but immediately notes these are rarely useful beyond perhaps bias vectors ([PyTorch Weight Initialization Demystified - Novita AI Blog](https://blogs.novita.ai/pytorch-weight-initialization-demystified/#:~:text=things%20centered%20around%20zero%20and,swap%20out%20zeros%20for%20ones)). This source is practical for those using PyTorch – it connects the theory to actual library calls and reinforces the reasoning behind those calls with intuitive phrasing. It doesn’t present new results but acts as a **developer-friendly summary** of the default strategies (Xavier, He, etc.) and when to use them.

**9. Comet ML Blog (2019) – “Selecting the right weight initialization for your deep neural network.”** ([Selecting the right weight initialization for your deep neural network - Comet](https://www.comet.com/site/blog/selecting-the-right-weight-initialization-for-your-deep-neural-network/#:~:text=Improperly%20initialized%20weights%20can%20negatively,propagation%20%28see%20more%C2%A0here)) – This blog (part 1 of a series) discusses why initialization is crucial and surveys current research (as of 2019). It highlights that improper initialization leads to vanishing or exploding gradients, slowing or preventing convergence ([Selecting the right weight initialization for your deep neural network - Comet](https://www.comet.com/site/blog/selecting-the-right-weight-initialization-for-your-deep-neural-network/#:~:text=Improperly%20initialized%20weights%20can%20negatively,propagation%20%28see%20more%C2%A0here)). The post is written for a broad audience and doesn’t delve into derivations, but it provides context and links to further reading. It notes that weight initialization was an active area of research, referencing techniques like Xavier/He and newer approaches (it likely mentions things like orthogonal init, initialization for GANs or normalization techniques). The blog’s value is mostly conceptual: it reiterates that among hyperparameters, the initial weights *“are incredibly important”* because they affect the initial trajectory of training ([Selecting the right weight initialization for your deep neural network - Comet](https://www.comet.com/site/blog/selecting-the-right-weight-initialization-for-your-deep-neural-network/#:~:text=The%20weight%20initialization%20technique%20you,therefore%2C%20the%20effectiveness%20of%20training)). It also points out that certain architectures (like ResNets) were essentially solutions to initialization and training difficulties ([Selecting the right weight initialization for your deep neural network - Comet](https://www.comet.com/site/blog/selecting-the-right-weight-initialization-for-your-deep-neural-network/#:~:text=contributing%20to%20the%C2%A0vanishing%C2%A0or%C2%A0exploding%20gradient%20problem,propagation%20%28see%20more%C2%A0here)), indirectly emphasizing how initialization and network design interplay. While it may not add details beyond what’s in other sources, it serves as a **confirmation from industry** that choosing the right initializer (Glorot vs He, etc.) can determine “whether [the network] converges at all” ([Selecting the right weight initialization for your deep neural network - Comet](https://www.comet.com/site/blog/selecting-the-right-weight-initialization-for-your-deep-neural-network/#:~:text=The%20weight%20initialization%20technique%20you,therefore%2C%20the%20effectiveness%20of%20training)). It encourages practitioners to pay attention to initialization rather than sticking with defaults blindly.

**10. Stack Exchange (2016) – “Why Initialize a Neural Network with Random Weights?”** ([Symmetry Breaking versus Zero Initialization - DeepLearning.AI](https://community.deeplearning.ai/t/symmetry-breaking-versus-zero-initialization/16061#:~:text=DeepLearning,recommends%20and%20randomly%20initialize)) ([Initializing neural networks - DeepLearning.AI](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=Initializing%20neural%20networks%20,)) – (Community Q&A, summarized) This Q&A thread addresses the fundamental question of **why not initialize all weights to zero**. The top answers explain the **symmetry breaking principle**: if all weights are identical (zero or any constant), then all neurons in a layer will behave identically for all inputs, receive identical gradients, and thus stay identical throughout training ([Symmetry Breaking versus Zero Initialization - DeepLearning.AI](https://community.deeplearning.ai/t/symmetry-breaking-versus-zero-initialization/16061#:~:text=DeepLearning,recommends%20and%20randomly%20initialize)) ([What does it mean to "break symmetry"? in the context of neural ...](https://stackoverflow.com/questions/59638646/what-does-it-mean-to-break-symmetry-in-the-context-of-neural-network-programm#:~:text=When%20all%20initial%20values%20are,the%20same%20gradient%2C%20and)). One answer succinctly states that with zero init, *“the gradients for all neurons will be the same… resulting in no learning”* ([Zero Initialization In Neural Networks | Restackio](https://www.restack.io/p/neural-networks-answer-zero-initialization-cat-ai#:~:text=Zero%20Initialization%20In%20Neural%20Networks,%C2%B7%20Learning%20Dynamics%3A%20The)). They also mention that even any constant initialization is just as bad in this regard. This resource, while not a formal blog, is included because it captures the consensus reasoning in simple terms and is often referenced for newcomers. It also touches on the idea that even if weights are random but too small, training can be **extremely slow** (because gradients are tiny) ([Initializing neural networks - DeepLearning.AI](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=Initializing%20neural%20networks%20,)), or if too large, gradients can explode – reinforcing why schemes like Xavier/He that pick a proper scale are needed. In summary, the community answer emphasizes: *randomness breaks symmetry, and scale matters*. This aligns perfectly with the motivations behind all the initialization strategies discussed above.

*(The above online resources complement the academic papers by providing intuitive explanations, code-level guidance, and demonstrations. They uniformly stress the need to avoid trivial inits (all zeros) and to choose Xavier/He/LeCun according to the activation to maintain stable signal propagation.)*

## Comparison of Initialization Methods

**Glorot (Xavier) vs He Initialization:** Both aim to keep activations and gradients in a reasonable range as networks deepen. **Xavier initialization** assumes roughly linear activation around 0 (or symmetric positive/negative outputs like tanh) and chooses the variance as a compromise between preserving forward and backward flow ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Glorot%20initialization%20,variance%20during%20the%20backward%20pass)). It works well for saturating activations in practice, significantly reducing the training difficulty compared to naive initializations ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=As%20you%20can%20see%2C%20in,was%20later%20refined%20into%20Kaiming)). However, for ReLU (which outputs zero for half the inputs, breaking the symmetry assumption), Xavier can lead to gradual variance drop layer by layer (effectively half the neurons off) ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)). **He initialization** fixes this by doubling the variance (for ReLU, variance $2/n_{in}$) ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=There%20is%20only%20one%20tiny,of%20the%20weights%20by%202)), ensuring the active neurons’ outputs have comparable magnitude to the previous layer’s. The difference is evident empirically: deep ReLU nets often *fail to train with Xavier* but train well with He ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood)). Thus, He init is theoretically and empirically more effective for ReLU/LeakyReLU, while Xavier is usually recommended for tanh/sigmoid networks ([Weight Initialization Techniques in Neural Networks | Pinecone](https://www.pinecone.io/learn/weight-initialization/#:~:text=,to%20use%20the%20He%20initialization)) ([Initializing neural networks - deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/#:~:text=This%20is%20a%20theoretical%20justification,as%20the%20one%20for%20tanh)). Both methods are widely used and have similar computational cost (just setting the random initialization scale). In terms of impact, these initializations were key enablers in the late-2000s/early-2010s that allowed networks to go from a few layers to tens of layers deep without layer-wise pre-training ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=In%20this%20post%2C%20I%E2%80%99ll%20walk,of%20two%20very%20significant%20papers)).

**LeCun Initialization:** This can be seen as a precursor to Xavier. It focuses only on preserving forward variance (fan-in) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=It%20samples%20each%20entry%20in,1)). For activations like sigmoid which tend to squash outputs, LeCun’s fan-in initialization keeps inputs to each neuron small enough to stay in the linear regime initially, which was found to speed up convergence even in shallow nets. Xavier actually generalizes LeCun’s idea to also consider backward flow; indeed if fan-in = fan-out, LeCun and Xavier formulas coincide ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=%7B%5Cdisplaystyle%20n_%7Bl,the%20same%20as%20LeCun%20initialization)). Empirically, LeCun init is especially relevant for SELU activation (as in SNNs), where it is part of a theoretically derived solution to maintain normalization through depth ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=and%2C%20therefore%20cannot%20exploit%20many,This)). In modern usage, if one uses SELU, frameworks like Keras default to `lecun_normal` initialization. For ReLU networks, LeCun’s $1/n_{in}$ variance would be too low (half the neurons off would halve variance each layer), so He’s $2/n_{in}$ is preferred. Conversely, using He init for a sigmoid network might be too high variance, causing saturation. So LeCun, Glorot, and He form a continuum – LeCun < Glorot < He in terms of variance – each optimal for a certain activation’s characteristics (sigmoid/tanh, linear, ReLU respectively) ([Weight Initialization Schemes - Xavier (Glorot) and He | Mustafa Murat ARAT](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init#:~:text=Activation%20Function%20Uniform%20Distribution%20%5B,out)).

**Orthogonal Initialization:** This method stands out because it’s not just about the variance of individual weights, but the structure of the weight *matrix*. An orthonormal weight matrix ensures the entire vector of activations neither expands nor contracts in length (assuming linear activation) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=%28Saxe%20et%20al.%202013%29,10)). Theoretical works ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=orthogonal%20matrices%20%2C%20multiplied%20by,10)) ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Classification,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)) show this can stabilize training dynamics for very deep or recurrent networks in ways that scalar variance tuning (Xavier/He) might not fully address (particularly in preserving correlations, not just magnitudes). Empirically, orthogonal init alone (with appropriate variance scaling) performs on par with Xavier/He for many feed-forward nets, and can be superior when depth is extreme or when trying to train without normalization. For example, ResNets and LSTMs often benefit from orthogonal kernel initialization to boost gradient flow. However, orthogonal initialization is more computationally involved (QR decomposition of a random matrix) and only applies to layers where a square-ish weight matrix is present (it’s defined for fully-connected or convolutional kernels, but not typically used for embeddings or batchnorm parameters, etc.). In practice, one might use orthogonal init in conjunction with He/Glorot scaling (e.g., initialize an orthogonal matrix then multiply by $\sqrt{2/n_{in}}$ for a ReLU layer – many libraries do this). Orthogonal init shines in cases like RNNs or very deep linear networks ([Weight initialization - Wikipedia](https://en.wikipedia.org/wiki/Weight_initialization#:~:text=Deep%20into%20Rectifiers%3A%20Surpassing%20Human,Ballard%2C%20Andy%3B%20Desjardins%2C%20Guillaume%3B%20Swirszcz)), ensuring training doesn’t stagnate as depth/time grows. In contrast, for moderate-depth networks, Xavier/He (which are simpler to implement) usually suffice and yield similar results.

**Zero Initialization:** Initializing all weights to zero (or any constant) is a **bad idea in almost all cases** ([Weight Initialization in Neural Net | by Samarth Gupta | Artificial Intelligence in Plain English](https://ai.plainenglish.io/weight-initialization-in-neural-net-d4d3bd5789af#:~:text=This%20blog%20covers%20weight%20initialization,I%20have%20some%20cases)) ([What does it mean to "break symmetry"? in the context of neural ...](https://stackoverflow.com/questions/59638646/what-does-it-mean-to-break-symmetry-in-the-context-of-neural-network-programm#:~:text=What%20does%20it%20mean%20to,the%20same%20gradient%2C%20and)). Theoretically, it destroys the symmetry between neurons – every neuron in a layer will receive the same gradient update, so they remain identical forever ([What does it mean to "break symmetry"? in the context of neural ...](https://stackoverflow.com/questions/59638646/what-does-it-mean-to-break-symmetry-in-the-context-of-neural-network-programm#:~:text=What%20does%20it%20mean%20to,the%20same%20gradient%2C%20and)). This means effectively you’re training a network with many redundant neurons that all do the same thing, wasting capacity. Empirically, a network with zero-initialized weights won’t learn anything non-trivial; it might as well have one neuron per layer. The only time you might initialize weights to zero is in specific architectures where symmetry is broken elsewhere – for instance, Fixup initialization sets some last-layer weights in residual blocks to zero **on purpose** ([How does fix-up initialization avoid prevent the neurons from ...](https://stats.stackexchange.com/questions/582809/how-does-fix-up-initialization-avoid-prevent-the-neurons-from-updating-in-the-ex#:~:text=How%20does%20fix,deep%20residual%20network%20without%20normalization)), but there the other branch has non-zero weights so the network isn’t symmetric. Similarly, biases can often be zero-initialized safely because having all biases zero doesn’t create symmetry (biases are per neuron, not connecting neurons). In fact, it’s common to see bias initialized to 0 and only weights initialized with Xavier/He. In summary, **never initialize all weights to zero** – random initialization is essential for diversity. As one source humorously put it, *“this is a big no-no! 🚫 When weights are zero, they don't get updated during training.” ([Xavier & He Initialization | Deep Learning basics - YouTube](https://www.youtube.com/watch?v=LKWatKGRZLI#:~:text=Xavier%20%26%20He%20Initialization%20,The%20network))】. All the advanced methods (LeCun, Glorot, He, orthogonal) assume random initialization; they just prescribe the right distribution to draw those random numbers from.

**Overall**, the choice of initialization method strongly affects trainability of deep models. **Xavier/Glorot** is a good default for deep networks with symmetric activations (tanh, linear) – it was key to training ~5-10 layer networks reliabl ([How to initialize deep neural networks? Xavier and Kaiming initialization • Pierre Ouannes](https://pouannes.github.io/blog/initialization/#:~:text=As%20you%20can%20see%2C%20in,was%20later%20refined%20into%20Kaiming))】. **He/Kaiming** initialization is crucial for ReLU networks – it enabled very deep CNNs in 2015 (like ResNets to go 100+ layers) by avoiding vanishing gradients in those rectified linear unit ([[1704.08863] On weight initialization in deep neural networks](https://ar5iv.org/abs/1704.08863#:~:text=In%20an%20important%20follow%20up,other%20is%20not%20fully%20understood))】. **LeCun’s init** is slightly older but remains relevant for specific cases like SELUs or shallow nets where keeping things small is enough. **Orthogonal init** provides an extra boost for extreme depths or recurrent nets, often used in conjunction with the others (it ensures directionality is preserved, while Xavier/He ensure magnitude is right). And as a baseline, **Zero init is almost always harmful** due to symmetry – all the above schemes use randomization to break symmetry. Modern libraries implement these strategies, so practitioners usually just call the appropriate initializer. But understanding their differences is important: for example, if one were to switch a network’s activation from ReLU to SELU, one must also switch from He init to LeCun init (as SELU requires $1/n$ scaling and using He’s $2/n$ would cause divergence ([Self-Normalizing Neural Networks - SERP AI](https://serp.ai/posts/self-normalizing-neural-networks/#:~:text=Self,achieve%20superior%20performance%20across)) ([[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515#:~:text=representations,to%20make%20learning%20highly%20robust))】. Each method has a theoretical foundation preventing gradient problems, and empirical benchmarks consistently show that using the correct initialization can improve convergence speed and final accuracy. As the saying from Mishkin & Matas goes, sometimes “**all you need is a good init**” to train even very deep networks successfull ([[1511.06422] All you need is a good init](https://ar5iv.labs.arxiv.org/html/1511.06422#:~:text=Experiment%20with%20different%20activation%20functions,%28%201))】.

